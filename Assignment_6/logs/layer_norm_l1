=============== Running layer_norm_l1 Model ===============
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 26, 26]              72
              ReLU-2            [-1, 8, 26, 26]               0
         LayerNorm-3            [-1, 8, 26, 26]          10,816
           Dropout-4            [-1, 8, 26, 26]               0
            Conv2d-5           [-1, 16, 24, 24]           1,152
              ReLU-6           [-1, 16, 24, 24]               0
         LayerNorm-7           [-1, 16, 24, 24]          18,432
           Dropout-8           [-1, 16, 24, 24]               0
         MaxPool2d-9           [-1, 16, 12, 12]               0
           Conv2d-10            [-1, 8, 12, 12]             128
           Conv2d-11           [-1, 16, 10, 10]           1,152
             ReLU-12           [-1, 16, 10, 10]               0
        LayerNorm-13           [-1, 16, 10, 10]           3,200
          Dropout-14           [-1, 16, 10, 10]               0
           Conv2d-15             [-1, 16, 8, 8]           2,304
             ReLU-16             [-1, 16, 8, 8]               0
        LayerNorm-17             [-1, 16, 8, 8]           2,048
          Dropout-18             [-1, 16, 8, 8]               0
           Conv2d-19             [-1, 16, 6, 6]           2,304
             ReLU-20             [-1, 16, 6, 6]               0
        LayerNorm-21             [-1, 16, 6, 6]           1,152
          Dropout-22             [-1, 16, 6, 6]               0
        AvgPool2d-23             [-1, 16, 1, 1]               0
           Conv2d-24             [-1, 16, 1, 1]             256
             ReLU-25             [-1, 16, 1, 1]               0
        LayerNorm-26             [-1, 16, 1, 1]              32
          Dropout-27             [-1, 16, 1, 1]               0
           Conv2d-28             [-1, 10, 1, 1]             160
================================================================
Total params: 43,208
Trainable params: 43,208
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.57
Params size (MB): 0.16
Estimated Total Size (MB): 0.74
----------------------------------------------------------------
None
Epoch1 : Loss=18.524839401245117  Accuracy=98.75 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.39it/s]

Test set: Average loss: 0.0153, Accuracy: 9954/10000 (99.54%)

Epoch2 : Loss=17.46906280517578  Accuracy=98.65 Batch_id=937: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:01<00:00, 15.22it/s]

Test set: Average loss: 0.0215, Accuracy: 9929/10000 (99.29%)

Epoch3 : Loss=16.157567977905273  Accuracy=97.99 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:01<00:00, 15.35it/s]

Test set: Average loss: 0.0326, Accuracy: 9897/10000 (98.97%)

Epoch4 : Loss=14.733323097229004  Accuracy=96.73 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.44it/s]

Test set: Average loss: 0.0733, Accuracy: 9782/10000 (97.82%)

Epoch5 : Loss=13.118462562561035  Accuracy=96.34 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.42it/s]

Test set: Average loss: 0.0481, Accuracy: 9872/10000 (98.72%)

Epoch6 : Loss=11.404292106628418  Accuracy=96.06 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.46it/s]

Test set: Average loss: 0.0565, Accuracy: 9839/10000 (98.39%)

Epoch7 : Loss=9.901098251342773  Accuracy=96.11 Batch_id=937: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.52it/s]

Test set: Average loss: 0.0546, Accuracy: 9847/10000 (98.47%)

Epoch8 : Loss=8.213054656982422  Accuracy=96.23 Batch_id=937: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:01<00:00, 15.23it/s]

Test set: Average loss: 0.0480, Accuracy: 9867/10000 (98.67%)

Epoch9 : Loss=6.383682727813721  Accuracy=96.24 Batch_id=937: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.59it/s]

Test set: Average loss: 0.0442, Accuracy: 9886/10000 (98.86%)

Epoch10 : Loss=5.0258097648620605  Accuracy=96.33 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.55it/s]

Test set: Average loss: 0.0481, Accuracy: 9873/10000 (98.73%)

Epoch11 : Loss=3.4014501571655273  Accuracy=96.34 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.56it/s]

Test set: Average loss: 0.0594, Accuracy: 9837/10000 (98.37%)

Epoch12 : Loss=2.117151975631714  Accuracy=95.89 Batch_id=937: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.58it/s]

Test set: Average loss: 0.0897, Accuracy: 9750/10000 (97.50%)

Epoch13 : Loss=1.2800891399383545  Accuracy=94.73 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.62it/s]

Test set: Average loss: 0.0778, Accuracy: 9766/10000 (97.66%)

Epoch14 : Loss=0.9148546457290649  Accuracy=93.13 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.48it/s]

Test set: Average loss: 0.0920, Accuracy: 9738/10000 (97.38%)

Epoch15 : Loss=0.6153278946876526  Accuracy=93.59 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.45it/s]

Test set: Average loss: 0.0917, Accuracy: 9739/10000 (97.39%)

Epoch16 : Loss=0.6314675807952881  Accuracy=93.97 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.43it/s]

Test set: Average loss: 0.0734, Accuracy: 9778/10000 (97.78%)

Epoch17 : Loss=0.5034530758857727  Accuracy=94.80 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:02<00:00, 15.04it/s]

Test set: Average loss: 0.0701, Accuracy: 9792/10000 (97.92%)

Epoch18 : Loss=0.5952659845352173  Accuracy=95.78 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:00<00:00, 15.40it/s]

Test set: Average loss: 0.0506, Accuracy: 9853/10000 (98.53%)

Epoch19 : Loss=0.3843485414981842  Accuracy=96.69 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:01<00:00, 15.19it/s]

Test set: Average loss: 0.0337, Accuracy: 9911/10000 (99.11%)

Epoch20 : Loss=0.3655857741832733  Accuracy=97.33 Batch_id=937: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [01:03<00:00, 14.72it/s]

Test set: Average loss: 0.0312, Accuracy: 9906/10000 (99.06%)

=============== Finished layer_norm_l1 Model ===============

[[1.0326200155243437, 0.23582263537116652, 0.1495401360430141, 0.11960520753974536, 0.1002196011943683, 0.0909188337666767, 0.08349227341614775, 0.08270749899207243, 0.07603284858389577, 0.07021128495103682, 0.06194261688121489, 0.06132294277701257, 0.05988687460200547, 0.053979132582109846, 0.052041569652198666, 0.0492500412694391, 0.04662887092720944, 0.04290536514841822, 0.04000676401182394, 0.03980756262433963], [0.880412957434461, 0.19912471119036426, 0.1274949203519377, 0.10490072103860631, 0.08896566354490912, 0.08119525099947636, 0.07242440620510737, 0.06727456087360718, 0.06344958965573261, 0.06072563207425328, 0.05704442554958729, 0.05029130838975223, 0.05078155893498837, 0.04969976651100025, 0.046379380771688725, 0.043930601375525014, 0.04287830479929585, 0.038773518760126635, 0.03849190406189529, 0.03587881302032103], [0.8563447741747919, 0.2129967795339411, 0.1721463397915747, 0.16113138818807565, 0.15657879738832142, 0.1514610197446716, 0.1539223301988929, 0.14917317412690376, 0.15327800099235545, 0.1517105453423282, 0.15012213214834744, 0.14592373328628952, 0.1425834727038675, 0.13623505826296806, 0.13144258562642247, 0.1261014237082494, 0.11193434342042978, 0.09916345961987655, 0.08505090652232263, 0.07379565067263617], [0.039263826746891724, 0.0491465214728765, 0.07753263744088744, 0.12154381645561409, 0.1421084309469408, 0.15891817827492571, 0.1469866690227885, 0.16184068284134492, 0.1546479943238183, 0.15645872492160498, 0.14768644625833358, 0.1504442467570686, 0.13907435762761497, 0.14428878567620382, 0.1330651349997692, 0.11916052413854136, 0.10821849043502896, 0.08960467547343087, 0.0726307096716358, 0.06240629488582423], [0.03879128995876231, 0.04376034140472672, 0.06704256994558423, 0.11041154399978867, 0.12570499497523377, 0.13395446172552006, 0.13439608346233997, 0.13138498555717945, 0.13001738438930815, 0.12695871356493438, 0.12777994103880644, 0.13820258750598122, 0.17879795811291951, 0.23091667246367378, 0.21539069954425033, 0.20155660622616187, 0.17519483747862297, 0.1470625263466033, 0.11501929646671645, 0.09450616954621285], [0.10515411242074184, 0.14241859642093752, 0.15687349115980903, 0.16387131035939526, 0.16848391223786227, 0.16823452255571447, 0.16866297135824587, 0.1693056882746311, 0.16318249773543908, 0.1652600879012458, 0.16158122373327835, 0.16098745700234035, 0.1594133790868368, 0.15417395275173537, 0.14543595437262294, 0.13587292121536632, 0.12896613564405804, 0.11368623647544938, 0.09225349029752491, 0.08348634550625932]]
[[0.16391104865074158, 0.08445189120471477, 0.051227124351263045, 0.04758705487921834, 0.03185318441689015, 0.03645463607311249, 0.02954137354195118, 0.039858856266736985, 0.027375275874882938, 0.024699578556604682, 0.0226386333424598, 0.02116835485585034, 0.022117179615050553, 0.019603161224722863, 0.01907662729769945, 0.017832451616227627, 0.01803675547996536, 0.01725062527358532, 0.016195719092339276, 0.016102783134952187], [0.16004583947658538, 0.053815334635972975, 0.04151957876384258, 0.039053082912415264, 0.04001806019842625, 0.029330705958604812, 0.022746563950926064, 0.024090996803343295, 0.023355864910036325, 0.019786664731614292, 0.022353596377000212, 0.0180424957299605, 0.015886623371765017, 0.018274652200937273, 0.01965946852937341, 0.01543489820174873, 0.015186155985342339, 0.014374017923325301, 0.013108946957439185, 0.013093001246266067], [0.11100947923064232, 0.0761525661766529, 0.05524833001047373, 0.060353325551003215, 0.05278152132332325, 0.06215681073218584, 0.05107531307935715, 0.04876960295662284, 0.07443313016965986, 0.0627752049446106, 0.056848526014387606, 0.04955221184492111, 0.06789459210932255, 0.04336524857580662, 0.05336482293009758, 0.08389410805106164, 0.03200246174708009, 0.03129126515639946, 0.02120757378935814, 0.021056743229180573], [0.01753919976167381, 0.020737208126485347, 0.034364261029660705, 0.0729911850631237, 0.07245617391616106, 0.060242962662875654, 0.07583836698234081, 0.07636688716113567, 0.06823996465206146, 0.06181674026250839, 0.06476221807599068, 0.054297805893421175, 0.06991114974021911, 0.05692519671320915, 0.06283179159164429, 0.05592569018006325, 0.040858170208334925, 0.03013841795027256, 0.025818495887517928, 0.024662702004984022], [0.015251825804077088, 0.02150126438960433, 0.032637630432844164, 0.07326846045851708, 0.04810991585701704, 0.05647403722256422, 0.05455914711356163, 0.04803344227671623, 0.04418573059141636, 0.048133547541499136, 0.0594269298017025, 0.08972915066480637, 0.0778031099587679, 0.09201895017623901, 0.09174171875268221, 0.07337520279586315, 0.07010425091832877, 0.05059077240601182, 0.03371514778584242, 0.03116841760277748], [0.04111921744346619, 0.05194810107946396, 0.061799919533729555, 0.11561764612197876, 0.07206886674165726, 0.07136362778842449, 0.1512912381887436, 0.08565846865028143, 0.09140724180936813, 0.0635642515361309, 0.08632402412295341, 0.07218840223550796, 0.06533426528796554, 0.07619909875690938, 0.054842540474236014, 0.04440927650034428, 0.04354979823827743, 0.032637772227823734, 0.023806322252005338, 0.022654238133877517]]
[[68.59166666666667, 93.41166666666666, 95.70166666666667, 96.56333333333333, 97.13833333333334, 97.32833333333333, 97.555, 97.565, 97.76833333333333, 97.99166666666666, 98.19166666666666, 98.19166666666666, 98.285, 98.42666666666666, 98.47333333333333, 98.54, 98.60333333333334, 98.71666666666667, 98.85333333333334, 98.83], [74.325, 94.44333333333333, 96.265, 96.99333333333334, 97.36166666666666, 97.56833333333333, 97.81166666666667, 98.0, 98.095, 98.19166666666666, 98.31333333333333, 98.48, 98.455, 98.5, 98.605, 98.67, 98.735, 98.78666666666666, 98.83833333333334, 98.96666666666667], [76.05666666666667, 94.03, 94.86333333333333, 95.135, 95.305, 95.54333333333334, 95.34666666666666, 95.485, 95.455, 95.41833333333334, 95.56833333333333, 95.70833333333333, 95.74333333333334, 95.92666666666666, 96.12166666666667, 96.29666666666667, 96.715, 97.06666666666666, 97.49833333333333, 97.89333333333333], [98.815, 98.55666666666667, 97.74333333333334, 96.46333333333334, 95.88333333333334, 95.38166666666666, 95.72333333333333, 95.39, 95.545, 95.49333333333334, 95.735, 95.73666666666666, 96.01166666666667, 95.92333333333333, 96.25333333333333, 96.64, 96.98666666666666, 97.46666666666667, 98.00666666666666, 98.315], [98.79666666666667, 98.7, 98.04, 96.785, 96.38833333333334, 96.11, 96.16333333333333, 96.28, 96.29333333333334, 96.375, 96.39166666666667, 95.93833333333333, 94.78333333333333, 93.18333333333334, 93.64666666666666, 94.02166666666666, 94.85, 95.82833333333333, 96.74333333333334, 97.38333333333334], [97.00166666666667, 95.82, 95.44333333333333, 95.34666666666666, 95.22833333333334, 95.23833333333333, 95.16333333333333, 95.14333333333333, 95.40666666666667, 95.33333333333333, 95.39333333333333, 95.44666666666667, 95.46, 95.68833333333333, 95.97, 96.30166666666666, 96.545, 96.90666666666667, 97.535, 97.91833333333334]]
[[96.8, 97.59, 98.48, 98.62, 99.1, 98.91, 99.12, 98.86, 99.17, 99.28, 99.35, 99.32, 99.24, 99.4, 99.39, 99.45, 99.48, 99.47, 99.51, 99.52], [96.15, 98.44, 98.74, 98.86, 98.82, 99.11, 99.32, 99.3, 99.25, 99.31, 99.33, 99.46, 99.54, 99.44, 99.46, 99.5, 99.49, 99.55, 99.62, 99.62], [97.18, 97.69, 98.32, 98.17, 98.39, 98.08, 98.41, 98.45, 97.85, 98.04, 98.14, 98.6, 97.92, 98.55, 98.45, 97.27, 99.04, 99.01, 99.37, 99.35], [99.49, 99.42, 99.05, 97.79, 97.86, 98.35, 97.81, 98.16, 98.06, 98.3, 98.19, 98.41, 97.88, 98.53, 98.25, 98.46, 98.95, 99.3, 99.32, 99.34], [99.54, 99.29, 98.97, 97.82, 98.72, 98.39, 98.47, 98.67, 98.86, 98.73, 98.37, 97.5, 97.66, 97.38, 97.39, 97.78, 97.92, 98.53, 99.11, 99.06], [98.6, 98.41, 98.14, 96.78, 97.75, 97.96, 95.49, 97.44, 97.54, 98.21, 97.49, 98.14, 98.02, 97.59, 98.37, 98.63, 98.77, 99.04, 99.38, 99.4]]
